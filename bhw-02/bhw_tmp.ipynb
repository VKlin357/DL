{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3423879-1123-4899-8ef0-29a36a3a97c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model from scratch...\n",
      "Starting training from scratch (no old checkpoints)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 1/10]: 100%|██████████| 3062/3062 [06:27<00:00,  7.90it/s]\n",
      "That's 100 lines that end in a tokenized period ('.')\n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10:\n",
      "  Train Loss: 4.9388\n",
      "  Val   Loss: 4.0904\n",
      "  Val   BLEU: 2.99\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 2/10]: 100%|██████████| 3062/3062 [06:29<00:00,  7.86it/s]\n",
      "That's 100 lines that end in a tokenized period ('.')\n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/10:\n",
      "  Train Loss: 3.6802\n",
      "  Val   Loss: 2.9951\n",
      "  Val   BLEU: 15.83\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 3/10]: 100%|██████████| 3062/3062 [06:29<00:00,  7.87it/s]\n",
      "That's 100 lines that end in a tokenized period ('.')\n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/10:\n",
      "  Train Loss: 2.8969\n",
      "  Val   Loss: 2.4954\n",
      "  Val   BLEU: 24.53\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 4/10]: 100%|██████████| 3062/3062 [06:28<00:00,  7.89it/s]\n",
      "That's 100 lines that end in a tokenized period ('.')\n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/10:\n",
      "  Train Loss: 2.4556\n",
      "  Val   Loss: 2.2875\n",
      "  Val   BLEU: 28.34\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 5/10]: 100%|██████████| 3062/3062 [06:28<00:00,  7.88it/s]\n",
      "That's 100 lines that end in a tokenized period ('.')\n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/10:\n",
      "  Train Loss: 2.1711\n",
      "  Val   Loss: 2.1647\n",
      "  Val   BLEU: 30.70\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 6/10]: 100%|██████████| 3062/3062 [06:27<00:00,  7.89it/s]\n",
      "That's 100 lines that end in a tokenized period ('.')\n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/10:\n",
      "  Train Loss: 1.9660\n",
      "  Val   Loss: 2.0939\n",
      "  Val   BLEU: 31.32\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 7/10]: 100%|██████████| 3062/3062 [06:29<00:00,  7.87it/s]\n",
      "That's 100 lines that end in a tokenized period ('.')\n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/10:\n",
      "  Train Loss: 1.8062\n",
      "  Val   Loss: 2.0312\n",
      "  Val   BLEU: 32.39\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 8/10]: 100%|██████████| 3062/3062 [06:29<00:00,  7.87it/s]\n",
      "That's 100 lines that end in a tokenized period ('.')\n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/10:\n",
      "  Train Loss: 1.6727\n",
      "  Val   Loss: 2.0274\n",
      "  Val   BLEU: 33.33\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 9/10]: 100%|██████████| 3062/3062 [06:28<00:00,  7.89it/s]\n",
      "That's 100 lines that end in a tokenized period ('.')\n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/10:\n",
      "  Train Loss: 1.5570\n",
      "  Val   Loss: 2.0076\n",
      "  Val   BLEU: 33.44\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 10/10]: 100%|██████████| 3062/3062 [06:28<00:00,  7.88it/s]\n",
      "That's 100 lines that end in a tokenized period ('.')\n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/10:\n",
      "  Train Loss: 1.4525\n",
      "  Val   Loss: 2.0378\n",
      "  Val   BLEU: 33.13\n",
      "\n",
      "Training completed. Best Val BLEU: 33.44\n",
      "Generating translations for test data...\n",
      "Done. Translations are in 'test1.de-en.en'.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import sacrebleu\n",
    "import warnings\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"Converting mask\")\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EMBED_DIM = 512\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 6\n",
    "FF_DIM = 2048\n",
    "DROPOUT = 0.1\n",
    "LEARNING_RATE = 1e-4\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1.0\n",
    "MIN_FREQ = 2\n",
    "MAX_LEN = 100\n",
    "\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "BOS_TOKEN = \"<bos>\"\n",
    "EOS_TOKEN = \"<eos>\"\n",
    "\n",
    "TRAIN_DE_PATH = \"./train.de-en.de\"\n",
    "TRAIN_EN_PATH = \"./train.de-en.en\"\n",
    "VAL_DE_PATH   = \"./val.de-en.de\"\n",
    "VAL_EN_PATH   = \"./val.de-en.en\"\n",
    "TEST_DE_PATH  = \"./test1.de-en.de\"\n",
    "OUTPUT_TEST_PATH = \"test1.de-en.en\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_data(de_path, en_path=None):\n",
    "    with open(de_path, 'r', encoding='utf-8') as f_de:\n",
    "        data_de = [line.strip().split() for line in f_de]\n",
    "    if en_path is not None:\n",
    "        with open(en_path, 'r', encoding='utf-8') as f_en:\n",
    "            data_en = [line.strip().split() for line in f_en]\n",
    "        return data_de, data_en\n",
    "    return data_de\n",
    "\n",
    "train_de, train_en = load_data(TRAIN_DE_PATH, TRAIN_EN_PATH)\n",
    "val_de, val_en = load_data(VAL_DE_PATH, VAL_EN_PATH)\n",
    "test_de = load_data(TEST_DE_PATH, None)\n",
    "\n",
    "def build_vocab(tokenized_texts, min_freq=MIN_FREQ):\n",
    "    counter = Counter()\n",
    "    for tokens in tokenized_texts:\n",
    "        counter.update(tokens)\n",
    "    specials = [UNK_TOKEN, PAD_TOKEN, BOS_TOKEN, EOS_TOKEN]\n",
    "    filtered_tokens = [tok for tok, count in counter.items() if count >= min_freq and tok not in specials]\n",
    "    filtered_tokens.sort(key=lambda x: (-counter[x], x))\n",
    "    idx2token = specials + filtered_tokens\n",
    "    token2idx = {token: idx for idx, token in enumerate(idx2token)}\n",
    "    return idx2token, token2idx\n",
    "\n",
    "idx2de, de2idx = build_vocab(train_de, min_freq=MIN_FREQ)\n",
    "idx2en, en2idx = build_vocab(train_en, min_freq=MIN_FREQ)\n",
    "\n",
    "pad_idx = en2idx[PAD_TOKEN]\n",
    "bos_idx = en2idx[BOS_TOKEN]\n",
    "eos_idx = en2idx[EOS_TOKEN]\n",
    "unk_idx = en2idx[UNK_TOKEN]\n",
    "\n",
    "def numericalize(tokens, token2idx):\n",
    "    return [token2idx.get(tok, token2idx[UNK_TOKEN]) for tok in tokens]\n",
    "\n",
    "def prepare_data(src_texts, trg_texts, de2idx, en2idx, bos_idx, eos_idx):\n",
    "    data_src = []\n",
    "    data_trg = []\n",
    "    for s, t in zip(src_texts, trg_texts):\n",
    "        s_idx = numericalize(s, de2idx)\n",
    "        t_idx = numericalize(t, en2idx)\n",
    "        t_idx = [bos_idx] + t_idx + [eos_idx]\n",
    "        data_src.append(s_idx)\n",
    "        data_trg.append(t_idx)\n",
    "    return data_src, data_trg\n",
    "\n",
    "train_src, train_trg = prepare_data(train_de, train_en, de2idx, en2idx, bos_idx, eos_idx)\n",
    "val_src, val_trg = prepare_data(val_de, val_en, de2idx, en2idx, bos_idx, eos_idx)\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_list, trg_list):\n",
    "        super().__init__()\n",
    "        self.src_list = src_list\n",
    "        self.trg_list = trg_list\n",
    "    def __len__(self):\n",
    "        return len(self.src_list)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.src_list[idx], self.trg_list[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_list, trg_list = zip(*batch)\n",
    "    max_src_len = max(len(s) for s in src_list)\n",
    "    max_trg_len = max(len(t) for t in trg_list)\n",
    "    padded_src = []\n",
    "    padded_trg = []\n",
    "    for s, t in zip(src_list, trg_list):\n",
    "        s_padded = s + [de2idx[PAD_TOKEN]] * (max_src_len - len(s))\n",
    "        t_padded = t + [en2idx[PAD_TOKEN]] * (max_trg_len - len(t))\n",
    "        padded_src.append(s_padded)\n",
    "        padded_trg.append(t_padded)\n",
    "    return torch.tensor(padded_src, dtype=torch.long), torch.tensor(padded_trg, dtype=torch.long)\n",
    "\n",
    "train_dataset = TranslationDataset(train_src, train_trg)\n",
    "val_dataset = TranslationDataset(val_src, val_trg)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:, :seq_len, :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerTranslator(nn.Module):\n",
    "    def __init__(self, src_vocab_size, trg_vocab_size, embed_dim, nheads, num_encoder_layers, num_decoder_layers, ff_dim, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.src_vocab_size = src_vocab_size\n",
    "        self.trg_vocab_size = trg_vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.trg_embedding = nn.Embedding(trg_vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, dropout)\n",
    "        self.pos_decoder = PositionalEncoding(embed_dim, dropout)\n",
    "        self.transformer = nn.Transformer(d_model=embed_dim, nhead=nheads, num_encoder_layers=num_encoder_layers, num_decoder_layers=num_decoder_layers, dim_feedforward=ff_dim, dropout=dropout, batch_first=True)\n",
    "        self.fc_out = nn.Linear(embed_dim, trg_vocab_size)\n",
    "        self._init_weights()\n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.src_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.trg_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.fc_out.weight)\n",
    "    def make_src_key_padding_mask(self, src):\n",
    "        return (src == de2idx[PAD_TOKEN])\n",
    "    def make_trg_key_padding_mask(self, trg):\n",
    "        return (trg == en2idx[PAD_TOKEN])\n",
    "    def forward(self, src, trg):\n",
    "        src_pad_mask = self.make_src_key_padding_mask(src)\n",
    "        trg_pad_mask = self.make_trg_key_padding_mask(trg)\n",
    "        seq_len = trg.size(1)\n",
    "        nopeak_mask = torch.triu(torch.ones(seq_len, seq_len, device=src.device), diagonal=1).bool()\n",
    "        embedded_src = self.src_embedding(src) * math.sqrt(self.embed_dim)\n",
    "        embedded_src = self.pos_encoder(embedded_src)\n",
    "        embedded_trg = self.trg_embedding(trg) * math.sqrt(self.embed_dim)\n",
    "        embedded_trg = self.pos_decoder(embedded_trg)\n",
    "        out = self.transformer(src=embedded_src, tgt=embedded_trg, src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=trg_pad_mask, memory_key_padding_mask=src_pad_mask, tgt_mask=nopeak_mask)\n",
    "        logits = self.fc_out(out)\n",
    "        return logits\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_translation(model, src, max_len, bos_idx, eos_idx):\n",
    "    model.eval()\n",
    "    src_pad_mask = (src == de2idx[PAD_TOKEN])\n",
    "    src_embed = model.src_embedding(src) * math.sqrt(model.embed_dim)\n",
    "    src_embed = model.pos_encoder(src_embed)\n",
    "    memory = model.transformer.encoder(src_embed, src_key_padding_mask=src_pad_mask)\n",
    "    batch_size = src.size(0)\n",
    "    ys = torch.full((batch_size, 1), bos_idx, dtype=torch.long, device=src.device)\n",
    "    for _ in range(max_len - 1):\n",
    "        tgt_embed = model.trg_embedding(ys) * math.sqrt(model.embed_dim)\n",
    "        tgt_embed = model.pos_decoder(tgt_embed)\n",
    "        nopeak_mask = torch.triu(torch.ones(tgt_embed.size(1), tgt_embed.size(1), device=src.device), diagonal=1).bool()\n",
    "        tgt_pad_mask = (ys == pad_idx)\n",
    "        out = model.transformer.decoder(tgt_embed, memory, tgt_mask=nopeak_mask, memory_key_padding_mask=src_pad_mask, tgt_key_padding_mask=tgt_pad_mask)\n",
    "        prob = model.fc_out(out[:, -1])\n",
    "        next_word = prob.argmax(dim=-1, keepdim=True)\n",
    "        ys = torch.cat([ys, next_word], dim=1)\n",
    "        if (next_word == eos_idx).all():\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_hyps = []\n",
    "    all_refs = []\n",
    "    with torch.no_grad():\n",
    "        for src_batch, trg_batch in loader:\n",
    "            src_batch = src_batch.to(device)\n",
    "            trg_batch = trg_batch.to(device)\n",
    "            output = model(src_batch, trg_batch[:, :-1])\n",
    "            loss = criterion(output.reshape(-1, model.trg_vocab_size), trg_batch[:, 1:].reshape(-1))\n",
    "            total_loss += loss.item()\n",
    "            gen = generate_translation(model, src_batch, MAX_LEN, bos_idx, eos_idx)\n",
    "            for pred_seq, ref_seq in zip(gen, trg_batch):\n",
    "                pred_tokens = []\n",
    "                for idx in pred_seq.tolist():\n",
    "                    if idx == eos_idx:\n",
    "                        break\n",
    "                    if idx not in [bos_idx, pad_idx, eos_idx]:\n",
    "                        pred_tokens.append(idx2en[idx])\n",
    "                all_hyps.append(\" \".join(pred_tokens))\n",
    "                ref_tokens = []\n",
    "                for idx in ref_seq.tolist():\n",
    "                    if idx == eos_idx:\n",
    "                        break\n",
    "                    if idx not in [bos_idx, pad_idx, eos_idx]:\n",
    "                        ref_tokens.append(idx2en[idx])\n",
    "                all_refs.append(\" \".join(ref_tokens))\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    bleu_score = sacrebleu.corpus_bleu(all_hyps, [all_refs], tokenize='none').score\n",
    "    return avg_loss, bleu_score\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs):\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    best_bleu = 0.0\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        for src_batch, trg_batch in tqdm(train_loader, desc=f\"[Epoch {epoch}/{epochs}]\"):\n",
    "            src_batch = src_batch.to(device)\n",
    "            trg_batch = trg_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src_batch, trg_batch[:, :-1])\n",
    "            loss = criterion(output.reshape(-1, model.trg_vocab_size), trg_batch[:, 1:].reshape(-1))\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        val_loss, val_bleu = evaluate(model, val_loader, criterion)\n",
    "        print(f\"\\nEpoch {epoch}/{epochs}:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Val   Loss: {val_loss:.4f}\")\n",
    "        print(f\"  Val   BLEU: {val_bleu:.2f}\\n\")\n",
    "        if val_bleu > best_bleu:\n",
    "            best_bleu = val_bleu\n",
    "    print(f\"Training completed. Best Val BLEU: {best_bleu:.2f}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Initializing model from scratch...\")\n",
    "    model = TransformerTranslator(src_vocab_size=len(idx2de), trg_vocab_size=len(idx2en), embed_dim=EMBED_DIM, nheads=NUM_HEADS, num_encoder_layers=NUM_LAYERS, num_decoder_layers=NUM_LAYERS, ff_dim=FF_DIM, dropout=DROPOUT, pad_idx=pad_idx).to(device)\n",
    "    print(\"Starting training from scratch (no old checkpoints)...\")\n",
    "    train_model(model, train_loader, val_loader, N_EPOCHS)\n",
    "    print(\"Generating translations for test data...\")\n",
    "    test_src_list = []\n",
    "    for sent in test_de:\n",
    "        idxs = [de2idx.get(tok, unk_idx) for tok in sent]\n",
    "        test_src_list.append(idxs)\n",
    "    all_preds = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for src_idxs in test_src_list:\n",
    "            src_tensor = torch.tensor([src_idxs], dtype=torch.long, device=device)\n",
    "            gen_seq = generate_translation(model, src_tensor, MAX_LEN, bos_idx, eos_idx)[0]\n",
    "            pred_tokens = []\n",
    "            for idx in gen_seq.tolist():\n",
    "                if idx == eos_idx:\n",
    "                    break\n",
    "                if idx not in [bos_idx, pad_idx, eos_idx]:\n",
    "                    pred_tokens.append(idx2en[idx])\n",
    "            all_preds.append(\" \".join(pred_tokens))\n",
    "    with open(OUTPUT_TEST_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        for line in all_preds:\n",
    "            f.write(line + \"\\n\")\n",
    "    print(f\"Done. Translations are in '{OUTPUT_TEST_PATH}'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7fd833b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/myenv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
